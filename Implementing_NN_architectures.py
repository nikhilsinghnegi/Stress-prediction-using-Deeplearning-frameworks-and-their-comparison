# -*- coding: utf-8 -*-
"""Btp_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AwMJN62Ckhxj2ACzIEw08dTqxWD3KLnA
"""

#%% importing data files from the drive
from google.colab import drive
drive.mount('/content/gdrive')

!unzip gdrive/My\ Drive/my\ academics/BTP_2/PC-VR.zip

#%% imports
from warnings import filters
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from time import time 

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Conv2D
from keras.layers import BatchNormalization
from keras.layers import Conv2DTranspose
from keras.layers import Dense
from keras.layers import Activation
from keras.layers import Add
from keras.layers import ReLU
from keras.layers import GlobalAveragePooling2D
from keras.layers import multiply
from keras.layers import Reshape

print("done imports")

#%% Loading the Data and Preprocessing
#grid size
rectangle_x = 0.01
rectangle_y = 0.01

#dataset size
data_set = 1000

#Dimensions for input data
n_x = 79 #the carttesian map divisions
n_y = 79

grid_dist_x = rectangle_x/n_x
grid_dist_y = rectangle_y/n_y

m_x = int(n_x + 1)
m_y = int(n_y + 1)

#initialize data matrices
data_cart_new = np.zeros((data_set,m_x,m_y)) #geometry matrix
data_stress_new = np.zeros((data_set,m_x,m_y)) #stress matrix
one_cart_new = np.zeros((data_set,m_x,m_y)) #Uni-geometry matrix for clean module
one_stress_new = np.zeros((data_set,m_x,m_y)) #Uni-stress matrix for clean module

#record time
start_1 = time()

#reading data from the ABAQUS analysis results files
for i in range(data_set):
    idx = str(i+1)
    txt_cart = '//content//fix_hole_hollow_small//Composite_uniform_SDF_Cart_'+ idx + '.dat'
    txt_stress = '//content//fix_hole_hollow_small//Composite_uniform_Stress_Cart_'+ idx + '.dat' 
    one_cart = '//content//fix_hole_hollow_small//One_SDF_cart_'+ idx + '.dat'
    one_stress = '//content//fix_hole_hollow_small//One_Stress_cart_'+ idx + '.dat'


    data_cart = np.loadtxt(txt_cart)
    data_stress = np.loadtxt(txt_stress)
    one_cart = np.loadtxt(one_cart)
    one_stress = np.loadtxt(one_stress)
    
    (m,n) = data_cart.shape
    

    for j in range(m):
        x = round(data_cart[j][0]/grid_dist_x)
        y = round(data_cart[j][1]/grid_dist_y)
        data_cart_new[i][y][x] = data_cart[j][2]  
        data_stress_new[i][y][x] = data_stress[j][2]
        one_cart_new[i][y][x] = one_cart[j][2]
        one_stress_new[i][y][x] = one_stress[j][2]


print("done with data loading")

#%% train test split random
X_train, X_test, Y_train, Y_test, one_cart_train, one_cart_test, one_stress_train, one_stress_test = train_test_split(data_cart_new, data_stress_new, one_cart_new, one_stress_new, test_size=0.2, random_state = 1)
X_test, X_cv, Y_test, Y_cv, one_cart_test, one_cart_cv, one_stress_test, one_stress_cv = train_test_split(X_test, Y_test, one_cart_test, one_stress_test, test_size = 0.5, random_state = 1) 


#reshaping data into matrices for neural network training (#sample, X, Y, feature) "-1(unknown dimension) inferred to be 800 here" "row wise flattening"
one_cart_train = tf.reshape(one_cart_train, [-1, m_x, m_y, 1])
one_stress_train = tf.reshape(one_stress_train, [-1, m_x, m_y, 1])
one_cart_test    = tf.reshape(one_cart_test,   [-1, m_x, m_y, 1])
one_stress_test  = tf.reshape(one_stress_test, [-1, m_x, m_y, 1])
one_cart_cv      = tf.reshape(one_cart_cv,     [-1, m_x, m_y, 1])
one_stress_cv    = tf.reshape(one_stress_cv,   [-1, m_x, m_y, 1])

input_train      = tf.reshape(X_train, [-1, m_x, m_y, 1])
output_train     = tf.reshape(Y_train, [-1, m_x, m_y, 1])
input_test  = tf.reshape(X_test, [-1, m_x, m_y, 1])
output_test = tf.reshape(Y_test, [-1, m_x, m_y, 1])
input_cv  = tf.reshape(X_cv, [-1, m_x, m_y, 1])
output_cv = tf.reshape(Y_cv, [-1, m_x, m_y, 1])


# Taking mean as reference geometry and reference stress contours (taking mean along axis 0, meaning for all dataset values)
sdf_ave = tf.reduce_mean(input_train, 0)
sdf_ave = tf.reshape(sdf_ave, [-1, m_x, m_y, 1])
stress_ave = tf.reduce_mean(output_train, 0)
stress_ave = tf.reshape(stress_ave, [-1, m_x, m_y, 1])

# calculating the geometry and stress difference contours for training, CV, test set
input_train_new = input_train - sdf_ave
output_train_new = output_train - stress_ave
# making ave value tensors to match with the size of input (axis 0 (of avg. contour) value '1' will get multiplied with a value '80')
[a1,b1,c1,d1] = input_train.shape 
stress_ave_train = tf.keras.backend.repeat_elements(stress_ave, rep = a1, axis = 0)

input_cv_new = input_cv - sdf_ave
[a2,b2,c2,d2] = input_cv.shape 
stress_ave_cv = tf.keras.backend.repeat_elements(stress_ave, rep = a2, axis = 0)

input_test_new = input_test - sdf_ave
[a3,b3,c3,d3] = input_test.shape 
stress_ave_test = tf.keras.backend.repeat_elements(stress_ave, rep = a3, axis = 0)

#%% Normalization module(from the original)
max_sdf     = np.max(input_train_new)
max_stress  = np.max(output_train_new)
min_sdf     = np.min(input_train_new)
min_stress  = np.min(output_train_new)

print(max_sdf)
print(min_sdf)
print(max_stress)
print(min_stress)

#normalizing
input_train_new = (input_train_new-min_sdf)/(max_sdf - min_sdf)
input_train_new = tf.math.multiply(input_train_new, one_cart_train) #for the clean module

input_cv_new = (input_cv_new-min_sdf)/(max_sdf - min_sdf)
input_cv_new = tf.math.multiply(input_cv_new, one_cart_cv) #for the clean module

input_test_new = (input_test_new-min_sdf)/(max_sdf - min_sdf)
input_test_new = tf.math.multiply(input_test_new, one_cart_test) #for the clean module
# taking the preprocessing time
t1 = time() - start_1
print('t1',t1)
# start recording the training time
start_2 = time()

#%% defining neural network different blocks

def conv_relu_block(x, filt, names):
    y = Conv2D(filters=filt, kernel_size=[2,2], strides = 2, padding='same', activation='linear', use_bias=True, name = names)(x)
    y = ReLU()(y)
    y = BatchNormalization()(y)
    return y

def se_block(x,filt,ratio=16): 
    init = x
    se_shape = (1, 1, filt)
    
    se = GlobalAveragePooling2D()(init)
    se = Reshape(se_shape)(se)
    se = Dense(filt // ratio, activation='relu', 
                               kernel_initializer='he_normal', 
                               use_bias=False)(se)
    se = Dense(filt, activation='sigmoid', 
                               kernel_initializer='he_normal', 
                               use_bias=False)(se)
    se = multiply([init, se])
    
    return se

def resnet_block(x,filt):

    y = Conv2D(filters=filt, kernel_size=[3,3], 
                               padding='same', activation='linear', 
                               use_bias=True)(x)
    y = ReLU()(y)
    y = BatchNormalization()(y)
    
    y = Conv2D(filters=filt, kernel_size=[3,3], 
                               padding='same', activation='linear',
                               use_bias=True)(y)
    y = ReLU()(y)
    y = BatchNormalization()(y)

    y = se_block(y,filt)
     
    y = Add()([y,x])
    
    return y

def deconv_norm_linear(x,filt,kernel,stride,names):
    
    y = Conv2DTranspose(filters=filt,kernel_size=kernel,
        strides=stride,padding='same',activation='linear', use_bias=True,
        name=names)(x)
    
    y = Activation(activation='linear')(y)
    
    y = BatchNormalization()(y)

    return y

def dense_block(x,filt,names):
    
    y = Dense(filt, activation='linear',
                              kernel_initializer='he_normal', use_bias=False,
                              name=names)(x)
    
    return y

print("defining DiNN blocks")

#%% Encoder-Decoder Neural Network Structure for DiNN

input_layer_1 = tf.keras.Input(shape= (m_x, m_y, 1), dtype=tf.float32)
input_layer_2 = tf.keras.Input(shape= (m_x, m_y, 1), dtype=tf.float32)
input_layer_3 = tf.keras.Input(shape= (m_x, m_y, 1), dtype=tf.float32)

conv_1 = conv_relu_block(input_layer_1, 32, 'conv1')
se_1 = se_block(conv_1, 32)

conv_2 = conv_relu_block(se_1, 64, 'conv2')
se_2 = se_block(conv_2, 64)

conv_3 = conv_relu_block(se_2, 128, 'conv3')
se_3 = se_block(conv_3, 128)

resnet_1 = resnet_block(se_3, 128)
resnet_2 = resnet_block(resnet_1, 128)
resnet_3 = resnet_block(resnet_2, 128)
resnet_4 = resnet_block(resnet_3, 128)
resnet_5 = resnet_block(resnet_4, 128)

deconv_0 = deconv_norm_linear(resnet_5, 128, [2,2], (2,2), 'deconv0') 
deconv_1 = deconv_norm_linear(deconv_0, 64, [2,2], (2,2), 'deconv1')
deconv_2 = deconv_norm_linear(deconv_1, 32, [2,2], (2,2), 'deconv2')
deconv_3 = deconv_norm_linear(deconv_2, 1, [2,2], (1,1), 'deconv3')

#denormalizing
deconv_3 = deconv_3*(max_stress-min_stress)+ min_stress

#we consider that large stress concentration exists if the stress
#ratio defined as Rσ = σmax/σmean is larger than 2

add = deconv_3 + input_layer_2
#followed by 2D De-Convolution block to smooth the prediction

deconv_4 = deconv_norm_linear(add, 1, [2,2], (1,1), 'deconv4')
deconv_4 = tf.keras.layers.ReLU()(deconv_4)
deconv_4 = tf.math.multiply(deconv_4, input_layer_3)

output_layer = deconv_4

model = tf.keras.models.Model(inputs = [input_layer_1, input_layer_2, input_layer_3], outputs = output_layer)

model.summary()

print("setting up NN")

#%% training the DiNN 

# optimiser
sgd = tf.keras.optimizers.SGD(learning_rate=1e-3, decay=1e-6, momentum=0.6, nesterov=True)

# Compile the model
model.compile(optimizer=sgd, loss=tf.keras.losses.mean_squared_error, metrics=['accuracy' ])

epoch = 150
# Fit (Train) the model
history = model.fit([input_train_new, stress_ave_train, one_stress_train], output_train, batch_size=256, epochs=epoch,\
                    validation_data=([input_cv_new, stress_ave_cv, one_stress_cv], output_cv))

# Evaluate the model on test set
predict = model.predict([input_test_new, stress_ave_test, one_stress_test])

# Evaludate the model on test set
score = model.evaluate([input_test_new, stress_ave_test, one_stress_test], output_test, verbose=1)
print('\n', 'Test accuracy', score)

# Record Neural Network Training and Prediction Time
t2 = time() - start_2 
print('t2',t2)
print("done with training and prediction")

#%% Generating history plots of training

# Summarize history for accuracy
fig_acc = plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy in training')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_acc.savefig('training_accuracy.png')

# Summarize history for loss
fig_loss = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_loss.savefig('training_loss.png')

#%% Evaluate prediction errors of DiNN

predict_output = predict[:, :, :, 0]
[p1,p2,p3] = predict_output.shape

# Initialize error matrices
max_real  = np.zeros(p1) 
min_real  = np.zeros(p1)

max_pred  = np.zeros(p1)
min_pred  = np.zeros(p1)

error_max = np.zeros(p1)
error_min = np.zeros(p1)

error_rate_max = np.zeros(p1)

# Loop through test sample to calculate average prediction error
for ip in range(p1):
                
    max_real[ip]   = np.max(Y_test[ip,:,:])
    max_pred[ip]   = np.max(predict[ip,:,:])
    
    error_max[ip]       = np.abs(max_real[ip] - max_pred[ip])
    error_rate_max[ip]  = error_max[ip] / max_real[ip]

    min_real[ip]   = np.min(Y_test[ip,:,:])
    min_pred[ip]   = np.min(predict[ip,:,:])
    error_min[ip]  = np.abs(min_real[ip] - min_pred[ip])

# Calculate Maximum Error Rate (MER), Maximum Error Average Rate and Minimum Average Rate
max_error_rate   = np.mean(error_rate_max)
max_error  = np.mean(error_max)
min_error  = np.mean(error_min)

# Print out predictin error results
print("max error average rate is:",  max_error_rate)
print("max error rate is:",  max_error)
print("min error rate is:",  min_error)

#%% Save the training results 
result = np.savetxt('result_summary.txt',
                    (max_error_rate,max_error,min_error,t1,t2,score[0]),
                    header='max error average rate, max error rate, min error rate, data_process, training, score')

# Plot CNN Model
tf.keras.utils.plot_model(model, to_file='model.png')
model.save('my_model.h5')

#%% Plot and generate graphs for test samples

# Geometry average plot
sdf_ave_plot = sdf_ave[0,:,:,0]

fig0_sdf_ave = plt.figure()
plt.title('SDF average')
plt.imshow(sdf_ave_plot, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig0_sdf_ave.savefig('sdf_ave.png')

# stress average plot
stress_ave_plot = stress_ave[0,:,:,0]

fig0_stress_ave = plt.figure()
plt.title('stress average')
plt.imshow(stress_ave_plot, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig0_stress_ave.savefig('stress_ave.png')

# The first test set
X_test_1 = input_test_new[0, :, :, 0]
Y_test_1 = output_test[0, :, :, 0]

fig1_sdf = plt.figure()
plt.title('SDF contour')
plt.imshow(X_test_1, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig1_sdf.savefig('SDF_1.png')

fig1_test = plt.figure()
plt.title('Stress contour')
plt.imshow(Y_test_1, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig1_test.savefig('Test_1.png')

predict_1 = predict[0, :, :, 0]
fig1_pred=plt.figure()
plt.title('Trained stress contour')
plt.imshow(predict_1, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig1_pred.savefig('Predict_1.png')

# The second test set
X_test_2 = input_test_new[3, :, :, 0]
Y_test_2 = output_test[3, :, :, 0]

fig2_sdf = plt.figure()
plt.title('SDF contour')
plt.imshow(X_test_2, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig2_sdf.savefig('SDF_2.png')

fig2_test = plt.figure()
plt.title('Stress contour')
plt.imshow(Y_test_2, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig2_test.savefig('Test_2.png')

predict_2 = predict[3,:,:,0]
fig2_pred=plt.figure()
plt.title('Trained stress contour')
plt.imshow(predict_2, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig2_pred.savefig('Predict_2.png')

# The third test set
X_test_3 = input_test_new[8, :, :, 0]
Y_test_3 = output_test[8, :, :, 0]

fig3_sdf = plt.figure()
plt.title('SDF contour')
plt.imshow(X_test_3, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig3_sdf.savefig('SDF_3.png')

fig3_test=plt.figure()
plt.title('Stress contour')
plt.imshow(Y_test_3, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig3_test.savefig('Test_3.png')

predict_3 = predict[8, :, :, 0]
fig3_pred=plt.figure()
plt.title('Trained stress contour')
plt.imshow(predict_3, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig3_pred.savefig('Predict_3.png')

# Output the stress difference
Y_diff = Y_test_1 - predict_1
fig_diff = plt.figure()
plt.title('Stress_difference')
plt.imshow(Y_diff,cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()
fig_diff.savefig('Stress_difference.png')

#%% Plot outputs of individual layers

# (1) plot the original one
input_image = input_train[0, :, :, 0]

plt.figure()
plt.title('input contour')
plt.imshow(input_image, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()

# (2) plot individual layer outputs (user can define the layer they want to output)
layer_names = ['conv1','conv2','conv3','deconv0','deconv1','deconv2','deconv3']

for layer_name in layer_names:
   
    inter_layer_model = tf.keras.models.Model(inputs=model.input,
                                          outputs=model.get_layer(layer_name).output)    
    inter_output = inter_layer_model.predict([input_test_new, stress_ave_test, one_stress_test])    
    ran = inter_output.shape[3]
    
    for j in range(ran):
        plot_inter_output = inter_output[0,:,:,j]
        layer_name_new = layer_name + str(j)
        
        fig = plt.figure()
        plt.title(layer_name_new)
        plt.imshow(plot_inter_output, cmap='rainbow')
        plt.colorbar()
        plt.grid(True) 
        
        name_layer = layer_name_new + '.png'
        fig.savefig(name_layer)
    
# (3) plot the real stress contour
output_image = output_train[0,:,:,0]

fig2 = plt.figure()
plt.title('stress contour')
plt.imshow(output_image, cmap='rainbow')
plt.colorbar()
plt.grid(True)
plt.show()

fig2.savefig('real_stress.png')

#%% Evaluate prediction errors of DiNN

predict_output = predict[:, :, :, 0]
[p1,p2,p3] = predict_output.shape

# Initialize error matrices
max_real  = np.zeros(p1) 
min_real  = np.zeros(p1)

max_pred  = np.zeros(p1)
min_pred  = np.zeros(p1)

error_max = np.zeros(p1)
error_min = np.zeros(p1)

error_rate_max = np.zeros(p1)

# Loop through test sample to calculate average prediction error
for ip in range(p1):
                
    max_real[ip]   = np.max(Y_test[ip,:,:])
    max_pred[ip]   = np.max(predict[ip,:,:])
    
    error_max[ip]       = np.abs(max_real[ip] - max_pred[ip])
    error_rate_max[ip]  = error_max[ip] / max_real[ip]

    min_real[ip]   = np.min(Y_test[ip,:,:])
    min_pred[ip]   = np.min(predict[ip,:,:])
    error_min[ip]  = np.abs(min_real[ip] - min_pred[ip])

# Calculate Maximum Error Rate (MER), Maximum Error Average Rate and Minimum Average Rate
max_error_rate   = np.mean(error_rate_max)
max_error  = np.mean(error_max)
min_error  = np.mean(error_min)

# Print out predictin error results
print("max error average rate is:",  max_error_rate)
print("max error rate is:",  max_error)
print("min error rate is:",  min_error)

#%% Save the training results 
result = np.savetxt('result_summary.txt',
                    (max_error_rate,max_error,min_error,t1,t2,score[0]),
                    header='max error average rate, max error rate, min error rate, data_process, training, score')

# Plot CNN Model
tf.keras.utils.plot_model(model, to_file='model.png')
model.save('my_model.h5')

# (2) plot individual layer outputs (user can define the layer they want to output)

layer_names = ['multiply_1','multiply_2','multiply_3','add','add_1','add_2','add_3', 'add_4']

for layer_name in layer_names:
   
    inter_layer_model = tf.keras.models.Model(inputs=model.input,
                                          outputs=model.get_layer(layer_name).output)    
    inter_output = inter_layer_model.predict([input_test_new, stress_ave_test, one_stress_test])    
    ran = inter_output.shape[3]
    
    for j in range(ran):
        plot_inter_output = inter_output[0,:,:,j]
        layer_name_new = layer_name + str(j)
        
        fig = plt.figure()
        plt.title(layer_name_new)
        plt.imshow(plot_inter_output, cmap='rainbow')
        plt.colorbar()
        plt.grid(True) 
        
        name_layer = layer_name_new + '.png'
        fig.savefig(name_layer)



"""**StressNet architecture**"""

#%% StressNet architecture

#%% defining neural network different blocks

def conv_relu_block(x, filt, names):
    y = tf.keras.layers.Conv2D(filters=filt, kernel_size=[2,2], strides = 2, padding='same', activation='linear', use_bias=True, name = names)(x)
    y = tf.keras.layers.BatchNormalization()(y)
    y = tf.keras.layers.ReLU()(y)
    return y

def se_block(x,filt,ratio=16):
    
    init = x
    se_shape = (1, 1, filt)
    
    se = tf.keras.layers.GlobalAveragePooling2D()(init)
    se = tf.keras.layers.Reshape(se_shape)(se)
    se = tf.keras.layers.Dense(filt // ratio, activation='relu', 
                               kernel_initializer='he_normal', 
                               use_bias=False)(se)
    se = tf.keras.layers.Dense(filt, activation='sigmoid', 
                               kernel_initializer='he_normal', 
                               use_bias=False)(se)
    se = tf.keras.layers.multiply([init, se])
    
    return se

def resnet_block(x,filt):

    y = tf.keras.layers.Conv2D(filters=filt, kernel_size=[3,3], 
                               padding='same', activation='linear', 
                               use_bias=True)(x)
    y = tf.keras.layers.ReLU()(y)
    y = tf.keras.layers.BatchNormalization()(y)
    
    y = tf.keras.layers.Conv2D(filters=filt, kernel_size=[3,3], 
                               padding='same', activation='linear',
                               use_bias=True)(y)
    y = tf.keras.layers.ReLU()(y)
    y = tf.keras.layers.BatchNormalization()(y)

    y = se_block(y,filt)
     
    y = tf.keras.layers.Add()([y,x])
    
    return y

def deconv_ReLU(x,filt,kernel,stride,names):
    
    y = tf.keras.layers.Conv2DTranspose(filters=filt,kernel_size=kernel,
        strides=stride,padding='same',activation='linear', use_bias=True,
        name=names)(x)
    
    y = tf.keras.layers.Activation(activation='ReLU')(y)
    
    y = tf.keras.layers.BatchNormalization()(y)

    return y

def dense_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, activation='linear',
                              kernel_initializer='he_normal', use_bias=False,
                              name=names)(x)
    
    return y

print("defining NN blocks")

#%% Encoder-Decoder Neural Network Structure for StressNet

input_layer_1 = tf.keras.Input(shape= (m_x, m_y, 1), dtype=tf.float32)


conv_1 = conv_relu_block(input_layer_1, 32, 'conv1')

conv_2 = conv_relu_block(conv_1, 64, 'conv2')

conv_3 = conv_relu_block(conv_2, 128, 'conv3')

resnet_1 = resnet_block(conv_3, 128)
resnet_2 = resnet_block(resnet_1, 128)
resnet_3 = resnet_block(resnet_2, 128)
resnet_4 = resnet_block(resnet_3, 128)
resnet_5 = resnet_block(resnet_4, 128)

deconv_1 = deconv_ReLU(resnet_5, 64, [2,2], (2,2), 'deconv1')
deconv_2 = deconv_ReLU(deconv_1, 32, [2,2], (2,2), 'deconv2')
deconv_3 = deconv_ReLU(deconv_2, 1, [2,2], (2,2), 'deconv3')


output_layer = deconv_3

model = tf.keras.models.Model(inputs = [input_layer_1], outputs = output_layer)

model.summary()

print("setting up StressNet")

#%% training the StressNet 
# optimiser

sgd = tf.keras.optimizers.SGD(learning_rate=1e-3, decay=1e-6, momentum=0.6, nesterov=True)

# Compile the model
model.compile(optimizer=sgd, loss=tf.keras.losses.mean_squared_error, metrics=['accuracy' ])

epoch = 150
# Fit (Train) the model
history = model.fit([input_train], output_train, batch_size=256, epochs=epoch,\
                    validation_data=([input_cv], output_cv))

# Evaluate the model on test set
predict = model.predict([input_test])

# Evaludate the model on test set
score = model.evaluate([input_test], output_test, verbose=1)
print('\n', 'Test accuracy', score)

# Record Neural Network Training and Prediction Time
#t2 = time() - start_2 

print("done with StressNet training and prediction")

# Summarize history for accuracy

fig_acc = plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy in training')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_acc.savefig('training_accuracy.png')

# Summarize history for loss
fig_loss = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_loss.savefig('training_loss.png')


#%% Evaluate prediction errors of StressNet

predict_output = predict[:, :, :, 0]
[p1,p2,p3] = predict_output.shape

# Initialize error matrices
max_real  = np.zeros(p1) 
min_real  = np.zeros(p1)

max_pred  = np.zeros(p1)
min_pred  = np.zeros(p1)

error_max = np.zeros(p1)
error_min = np.zeros(p1)

error_rate_max = np.zeros(p1)

# Loop through test sample to calculate average prediction error
for ip in range(p1):
                
    max_real[ip]   = np.max(Y_test[ip,:,:])
    max_pred[ip]   = np.max(predict[ip,:,:])
    
    error_max[ip]       = np.abs(max_real[ip] - max_pred[ip])
    error_rate_max[ip]  = error_max[ip] / max_real[ip]

    min_real[ip]   = np.min(Y_test[ip,:,:])
    min_pred[ip]   = np.min(predict[ip,:,:])
    error_min[ip]  = np.abs(min_real[ip] - min_pred[ip])

# Calculate Maximum Error Rate (MER), Maximum Error Average Rate and Minimum Average Rate
max_error_rate   = np.mean(error_rate_max)
max_error  = np.mean(error_max)
min_error  = np.mean(error_min)

# Print out predictin error results
print("max error average rate is:",  max_error_rate)
print("max error rate is:",  max_error)
print("min error rate is:",  min_error)

#%% Save the training results 
result = np.savetxt('result_summary1.txt',
                    (max_error_rate,max_error,min_error,t1,t2,score[0]),
                    header='max error average rate, max error rate, min error rate, data_process, training, score')

# Plot CNN Model
tf.keras.utils.plot_model(model, to_file='model1.png')
model.save('my_model1.h5')







"""**SCSNet architecture**"""

#%% SCSNet architecture

#%% defining neural network different blocks

def conv_relu_pool_block(x, filt, names):
    y = tf.keras.layers.Conv2D(filters=filt, kernel_size=[2,2], strides = 1, padding='same', activation='linear', use_bias=True, name = names)(x)
    y = tf.keras.layers.ReLU()(y)
    y = tf.keras.layers.MaxPool2D(pool_size=(2, 2),strides=None,padding='valid')(y)
    return y


def deconv_ReLU(x,filt,kernel,stride,names):
    
    y = tf.keras.layers.Conv2DTranspose(filters=filt,kernel_size=kernel,
        strides=stride,padding='same',activation='linear', use_bias=True,
        name=names)(x)
    y = tf.keras.layers.Activation(activation='ReLU')(y)
 
    return y

def dense_block(x,filt,names):
    
    y = tf.keras.layers.Dense(filt, activation='linear',
                              kernel_initializer='he_normal', use_bias=False,
                              name=names)(x)
    
    return y

print("defining SCSNet blocks")

#%% Encoder-Decoder Neural Network Structure for SCSNet

input_layer_1 = tf.keras.Input(shape= (m_x, m_y, 1), dtype=tf.float32)
input_layer_2 = tf.keras.Input(shape= (2), dtype=tf.float32)

conv_1 = conv_relu_pool_block(input_layer_1, 32, 'conv1')

conv_2 = conv_relu_pool_block(conv_1, 64, 'conv2')

[a,b,c,d] = conv_2.shape

reshape1 = tf.keras.layers.Reshape([b*c*d])(conv_2)

dense1 = dense_block(reshape1, 1024, 'dense1' )

dense2 = dense_block(dense1, 30, 'dense2') # None*30


val = tf.keras.layers.concatenate([dense2,input_layer_2], axis = 1)


dense3 = dense_block(val, 1024, 'dense3' )

dense4 = dense_block(dense3, b*c*d, 'dense4' )


reshape2 = tf.keras.layers.Reshape([b,c,d])(dense4)



deconv_1 = deconv_ReLU(reshape2, 32, [2,2], (2,2), 'deconv1')
deconv_2 = deconv_ReLU(deconv_1, 1, [2,2], (2,2), 'deconv2')


output_layer = deconv_2

model = tf.keras.models.Model(inputs = [input_layer_1, input_layer_2], outputs = output_layer)

model.summary()

print("setting up SCSNet")

[a1,b1,c1,d1] = input_train.shape
zeros1 = np.zeros((a1,2)) #for zero load matrix
zeros1 = tf.reshape(zeros1, [-1,2])
print(zeros1.shape)

[a2,b2,c2,d2] = input_cv.shape
zeros2 = np.zeros((a2,2)) #for zero load matrix
zeros2 = tf.reshape(zeros2, [-1,2])
print(zeros2.shape)

[a3,b3,c3,d3] = input_test.shape
zeros3 = np.zeros((a3,2)) #for zero load matrix
zeros3 = tf.reshape(zeros3, [-1,2])
print(zeros3.shape)

#%% training the SCSNet
# optimiser
sgd = tf.keras.optimizers.SGD(learning_rate=1e-3, decay=1e-6, momentum=0.6, nesterov=True)

# Compile the model
model.compile(optimizer=sgd, loss=tf.keras.losses.mean_squared_error, metrics=['accuracy' ])

epoch = 150
# Fit (Train) the model
history = model.fit([input_train, zeros1], output_train, batch_size=None, epochs=epoch,\
                    validation_data=([input_cv, zeros2], output_cv))

# Evaluate the model on test set
predict = model.predict([input_test, zeros3])

# Evaludate the model on test set
score = model.evaluate([input_test, zeros3], output_test, verbose=1)
print('\n', 'Test accuracy', score)

# Record Neural Network Training and Prediction Time
#t2 = time() - start_2 

print("done with training and prediction")

# Summarize history for accuracy

fig_acc = plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy in training')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_acc.savefig('training_accuracy.png')

# Summarize history for loss
fig_loss = plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
fig_loss.savefig('training_loss.png')


#%% Evaluate prediction errors of StressNet

predict_output = predict[:, :, :, 0]
[p1,p2,p3] = predict_output.shape

# Initialize error matrices
max_real  = np.zeros(p1) 
min_real  = np.zeros(p1)

max_pred  = np.zeros(p1)
min_pred  = np.zeros(p1)

error_max = np.zeros(p1)
error_min = np.zeros(p1)

error_rate_max = np.zeros(p1)

# Loop through test sample to calculate average prediction error
for ip in range(p1):
                
    max_real[ip]   = np.max(Y_test[ip,:,:])
    max_pred[ip]   = np.max(predict[ip,:,:])
    
    error_max[ip]       = np.abs(max_real[ip] - max_pred[ip])
    error_rate_max[ip]  = error_max[ip] / max_real[ip]

    min_real[ip]   = np.min(Y_test[ip,:,:])
    min_pred[ip]   = np.min(predict[ip,:,:])
    error_min[ip]  = np.abs(min_real[ip] - min_pred[ip])

# Calculate Maximum Error Rate (MER), Maximum Error Average Rate and Minimum Average Rate
max_error_rate   = np.mean(error_rate_max)
max_error  = np.mean(error_max)
min_error  = np.mean(error_min)

# Print out predictin error results
print("max error average rate is:",  max_error_rate)
print("max error rate is:",  max_error)
print("min error rate is:",  min_error)

#%% Save the training results 
result = np.savetxt('result_summary2.txt',
                    (max_error_rate,max_error,min_error,t1,t2,score[0]),
                    header='max error average rate, max error rate, min error rate, data_process, training, score')

# Plot CNN Model
tf.keras.utils.plot_model(model, to_file='model2.png')
model.save('my_model2.h5')


